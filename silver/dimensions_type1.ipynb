{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a06dd8b-8f7d-4c96-94f3-20c539a0592c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"logistics_catalog.bronze.customers_bronze\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ffcc44-da26-477e-b3d8-963e113d9946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# SILVER DIMENSIONS TABLE CREATION\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window   \n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "\n",
    "# Merge Function \n",
    "\n",
    "\n",
    "def type1_merge(src_df, target_table, key_col):\n",
    "    \n",
    "    if not spark.catalog.tableExists(target_table):\n",
    "        print(f\"Creating table first time → {target_table}\")\n",
    "        src_df.write.format(\"delta\").saveAsTable(target_table)\n",
    "\n",
    "    else:\n",
    "        print(f\"Merging into → {target_table}\")\n",
    "        delta_tbl = DeltaTable.forName(spark, target_table)\n",
    "\n",
    "        (\n",
    "            delta_tbl.alias(\"t\")\n",
    "            .merge(\n",
    "                src_df.alias(\"s\"),\n",
    "                f\"t.{key_col} = s.{key_col}\"\n",
    "            )\n",
    "            .whenMatchedUpdateAll()       \n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "\n",
    "# 1️⃣ CUSTOMER TABLE\n",
    "\n",
    "\n",
    "def load_customer():\n",
    "\n",
    "    bronze_table = \"logistics_catalog.bronze.customers_bronze\"\n",
    "    silver_table = \"logistics_catalog.silver.dim_customer\"\n",
    "\n",
    "    df = spark.read.table(bronze_table)\n",
    "\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"customer_id\") \\\n",
    "        .orderBy(col(\"_metadata.file_modification_time\").desc())\n",
    "\n",
    "    clean_df = (\n",
    "        df\n",
    "        .withColumn(\"customer_id\", upper(trim(col(\"customer_id\")))) \\\n",
    "        .withColumn(\"customer_name\", initcap(trim(col(\"customer_name\")))) \\\n",
    "        .withColumn(\"country\", upper(trim(col(\"country\")))) \\\n",
    "        .withColumn(\"city\", initcap(trim(col(\"city\")))) \n",
    "        .withColumn(\"rn\", row_number().over(window_spec))\n",
    "        .filter(\"rn = 1\")\n",
    "        .drop(\"rn\")\n",
    "        .fillna({\n",
    "        \"customer_name\": \"UNKNOWN\",\n",
    "        \"country\": \"UNKNOWN\",\n",
    "        \"city\": \"UNKNOWN\"\n",
    "        })\n",
    "        .dropDuplicates([\"customer_id\"])\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    type1_merge(clean_df, silver_table, \"customer_id\")\n",
    "\n",
    "\n",
    "\n",
    "# PRODUCT TABLE\n",
    "\n",
    "\n",
    "def load_product():\n",
    "\n",
    "    bronze_table = \"logistics_catalog.bronze.products_bronze\"\n",
    "    silver_table = \"logistics_catalog.silver.dim_product\"\n",
    "\n",
    "    df = spark.read.table(bronze_table)\n",
    "\n",
    "    window_spec = Window.partitionBy(\"product_id\") \\\n",
    "        .orderBy(col(\"_metadata.file_modification_time\").desc())\n",
    "\n",
    "    clean_df = (\n",
    "        df\n",
    "        .withColumn(\"product_id\", upper(trim(col(\"product_id\")))) \\\n",
    "        .withColumn(\"product_name\", initcap(trim(col(\"product_name\")))) \\\n",
    "        .withColumn(\"category\", upper(trim(col(\"category\"))))\n",
    "        .withColumn(\n",
    "        \"weight_kg\",\n",
    "        when(col(\"weight_kg\").cast(\"double\") <= 0, None)\n",
    "        .otherwise(col(\"weight_kg\").cast(\"double\"))\n",
    "        )\n",
    "        .fillna({\n",
    "        \"category\": \"UNKNOWN\",\n",
    "        \"weight_kg\": 0\n",
    "        })\n",
    "        .withColumn(\"rn\", row_number().over(window_spec))\n",
    "        .filter(\"rn = 1\")\n",
    "        .drop(\"rn\")\n",
    "        .dropDuplicates([\"product_id\"])\n",
    "        \n",
    "    )\n",
    "\n",
    "    type1_merge(clean_df, silver_table, \"product_id\")\n",
    "\n",
    "\n",
    "\n",
    "# CARRIER TABLE\n",
    "\n",
    "\n",
    "def load_carrier():\n",
    "\n",
    "    bronze_table = \"logistics_catalog.bronze.carrier_bronze\"\n",
    "    silver_table = \"logistics_catalog.silver.dim_carrier\"\n",
    "\n",
    "    df = spark.read.table(bronze_table)\n",
    "\n",
    "    window_spec = Window.partitionBy(\"carrier_id\") \\\n",
    "        .orderBy(col(\"_metadata.file_modification_time\").desc())\n",
    "\n",
    "    clean_df = (\n",
    "        df\n",
    "        .withColumn(\"carrier_id\", upper(trim(col(\"carrier_id\")))) \\\n",
    "        .withColumn(\"carrier_name\", initcap(trim(col(\"carrier_name\")))) \\\n",
    "        .withColumn(\"carrier_type\", upper(trim(col(\"carrier_type\")))) \\\n",
    "        .withColumn(\"country\", upper(trim(col(\"country\"))))\n",
    "        .fillna({\n",
    "        \"carrier_name\": \"UNKNOWN\",\n",
    "        \"carrier_type\": \"UNKNOWN\",\n",
    "        \"country\": \"UNKNOWN\"\n",
    "         })\n",
    "        .withColumn(\"rn\", row_number().over(window_spec))\n",
    "        .filter(\"rn = 1\")\n",
    "        .drop(\"rn\")\n",
    "        .dropDuplicates([\"carrier_id\"])\n",
    "       \n",
    "\n",
    "    )\n",
    "\n",
    "    type1_merge(clean_df, silver_table, \"carrier_id\")\n",
    "\n",
    "\n",
    "\n",
    "# FUNCTION EXECUTION\n",
    "\n",
    "\n",
    "print(\"Starting Dimension Loads...\")\n",
    "\n",
    "load_customer()\n",
    "load_product()\n",
    "load_carrier()\n",
    "\n",
    "print(\"All dimensions loaded successfully ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f180024a-f91d-4c30-8736-e8d2b7de55e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from logistics_catalog.silver.dim_customer"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7306022604394535,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "dimensions_type1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
